{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052c51c8-9aaf-4d7d-9d5b-5c76922c808a",
   "metadata": {},
   "source": [
    "# Normal NN with MSE loss (from some old test)\n",
    "\n",
    "    1. Train on OMM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6e8c05e-5116-4fd5-b982-3a67a3e97c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchsummary import summary\n",
    "import tqdm\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b719b-e014-4081-82c9-c2cdf695b326",
   "metadata": {},
   "source": [
    "# Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18b12ee6-c593-4aae-90ef-560b7260dac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "    use_cuda=True\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "    use_cuda=False\n",
    "cudnn.benchmark = True      #May train faster but cost more memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b244152b-4977-462f-b33b-ecd359e03cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATS = 'WL_23_WPH_short_CMBWL'\n",
    "\n",
    "GAUSSIAN_AUG_STRENTH = 0.1\n",
    "BATCH_SIZE           = 16\n",
    "LEARNING_RATE        = 0.0005\n",
    "NUM_EPOCH            = 200\n",
    "WEIGHT_DECAY_RATE    = 0.01\n",
    "SCHEDULER_FACTOR     = 0.3\n",
    "SCHEDULER_PATIENCE   = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e8b4182-c416-4146-9327-8b7d334478d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WL_3_WPH_short\n",
      "WL_23_WPH_short\n",
      "WL_23_WPH_short_CMBWL\n",
      "WPH\n",
      "CMBWL\n"
     ]
    }
   ],
   "source": [
    "DD = np.load('/global/cfs/cdirs/des/mgatti/CMB_lensing/DV/SBI_forecast/compression/compression_data_combined.npy',allow_pickle=True).item()\n",
    "stat = DD['stat']\n",
    "mask = DD['mask']\n",
    "target = DD['data']\n",
    "\n",
    "# wph get rid of stuff that's degenerate with wl2 and wl3\n",
    "# this defines \"WPH_short\"\n",
    "mask_l = np.array(16*[ True,  True,  True,  True, False, False,  True,  True,  True,\n",
    "        True,  True,  True, False, False, False, False,  True,  True,\n",
    "        True,  True, False, False, False, False,  True,  True, False,\n",
    "       False, False, False, False, False, False, False, False, False])\n",
    "mask_nbody_wph = np.hstack([np.array([False]*60),np.array([False]*120),mask_l])\n",
    "\n",
    "# these keys are DVs that don't exist. \n",
    "# These indices are defined to select from their corresponding odict ('original dictionary') DVs that do exist\n",
    "indict2 = dict()\n",
    "indict2['WL_23_WPH_short'] = np.concatenate( ( list( range(320) ), np.array( range(320, 1076))[mask_nbody_wph]) )\n",
    "indict2['WL_3_WPH_short'] = np.concatenate( ( list( range(160, 320) ), np.array( range(320, 1076))[mask_nbody_wph]) )\n",
    "indict2['WL_23_WPH_short_CMBWL'] = np.concatenate( ( list( range(320) ), np.array( range(320, 1076))[mask_nbody_wph], list(range(1076, 1108) )) )\n",
    "indict2['WPH'] = np.array( range(160, 916))[mask_nbody_wph]\n",
    "indict2['CMBWL'] = range(160, 192)\n",
    "\n",
    "odict = dict()\n",
    "odict['WL_3_WPH_short'] = 'WL_23_WPH'\n",
    "odict['WL_23_WPH_short'] = 'WL_23_WPH'\n",
    "odict['WL_23_WPH_short_CMBWL'] = 'WL_23_WPH_WCMBL'\n",
    "odict['WPH'] = 'WL_2_WPH'\n",
    "odict['CMBWL'] = 'WL_2_WCMBL'\n",
    "\n",
    "for key in odict.keys():\n",
    "    print(key)\n",
    "    stat[key] = stat[odict[key]].copy()\n",
    "    stat[key]['dv'] = stat[key]['dv'][:,indict2[key]]\n",
    "        \n",
    "\n",
    "swapind = np.array([0,1,2,3,4,13,14,15,9,10,11,12,5,6,7,8,16])   # om s8 w a e .    ob ns h ...   dm dz\n",
    "\n",
    "for key in stat.keys():\n",
    "    stat[key]['params'] = stat[key]['params'][:,swapind]\n",
    "\n",
    "# defining additional_mask that filters away some extreme values of w, A, eta\n",
    "parms = stat['WL_2']['params']\n",
    "extra =   (parms[:,3]<0.8)  & (parms[:,3]>0.2) &\\\n",
    "         (parms[:,4]>0.1) &   (parms[:,4]<0.9) \n",
    "\n",
    "additional_mask = (stat['WL_2']['params'][:,2]>0.1)&extra\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "615efc82-3f35-4f81-a928-9670afbccbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the pars/DV that will be used to train the compression (or whatever pre-processing).\n",
    "pars_train = np.array(stat[STATS]['params'][mask&additional_mask,:16])\n",
    "dv = np.array(stat[STATS]['dv'][mask&additional_mask,:])\n",
    "\n",
    "    # these are the pars/DV that will be used for the LFI step later on \n",
    "    # (so you apply whatever compression/preprocessing to these and give to NDE)\n",
    "pars_LFI = np.array(stat[STATS]['params'][(~mask)&additional_mask,:16])\n",
    "dv_LFI = np.array(stat[STATS]['dv'][(~mask)&additional_mask,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e07eb0-61e3-435b-98d2-c008d05efc5c",
   "metadata": {},
   "source": [
    "# Start Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f300b4d-f840-4293-88ad-5b4ba01c4df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# no normalization\n",
    "\n",
    "# train only on OMM\n",
    "pars_train = pars_train[:, 0:1]\n",
    "pars_LFI = pars_LFI[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2526b8ee-b505-40de-80c5-a1028aeab301",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking data set sizes torch.Size([8211, 608]) torch.Size([913, 608]) torch.Size([8187, 608])\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(pars_train)\n",
    "# split the sample for training ----------\n",
    "train_split, val_split = int(0.9*num_samples), int(0.1*num_samples)\n",
    "\n",
    "train_x, val_x = np.split(dv, [train_split])\n",
    "train_y, val_y = np.split(pars_train, [train_split])\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_x[:]), torch.from_numpy(train_y[:]))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(val_x[:]), torch.from_numpy(val_y))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(dv_LFI[:]), torch.from_numpy(pars_LFI))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('checking data set sizes',train_dataset.tensors[0].shape, val_dataset.tensors[0].shape, test_dataset.tensors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba29267-3140-47fe-826b-bd0f02d5368e",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fb2cabf-ed28-456b-a353-c6640ffdae69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                train_loader, \n",
    "                val_loader,\n",
    "                dataset_size, \n",
    "                val_size, \n",
    "                optimizer, \n",
    "                num_epochs):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for bi, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        train_losses_tracker.append(epoch_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print('Loss: {:.4f}'.format(epoch_loss))\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            # Iterate over data.\n",
    "            for bi, (inputs, labels) in enumerate(val_loader):\n",
    "                inputs = inputs.to(device, dtype=torch.float)\n",
    "                labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            epoch_val_loss = running_loss / val_size\n",
    "            valid_losses_tracker.append(epoch_val_loss)\n",
    "            scheduler.step(epoch_val_loss)\n",
    "        if epoch % 10 == 0:  \n",
    "            print('Val Loss: {:.4f}'.format(epoch_val_loss))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e80e942-ebc2-469d-8ec1-fc796ce6ee44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_test_error(model, test_loader,output=False, out_name='test', Y_min=0, Y_max=1, color='steelblue'):\n",
    "\n",
    "    g=[0]\n",
    "\n",
    "    test_loss1, test_loss2 = torch.zeros(len(g)).to(device), torch.zeros(len(g)).to(device)\n",
    "    test_loss, points = 0.0, 0\n",
    "\n",
    "    ## Model performance metrics on test set\n",
    "    num_maps=test_loader.dataset.tensors[0].shape[0]\n",
    "\n",
    "    # define the arrays containing the value of the parameters\n",
    "    params_true = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    params_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "    errors_NN   = np.zeros((num_maps,len(g)), dtype=np.float32)\n",
    "\n",
    "    model.eval()\n",
    "    for x, y in test_loader:\n",
    "        with torch.no_grad():\n",
    "            bs    = x.shape[0]         #batch size\n",
    "            if x.dtype == torch.float64:\n",
    "                x = x.float()\n",
    "            x     = x.to(device)       #send data to device\n",
    "            y     = y.to(device)  #send data to device\n",
    "            p     = model(x)           #prediction for mean and variance\n",
    "            y_NN  = p           #prediction for mean\n",
    "\n",
    "            # save results to their corresponding arrays\n",
    "            params_true[points:points+x.shape[0]] = y.cpu().numpy() \n",
    "            params_NN[points:points+x.shape[0]]   = y_NN.cpu().numpy()\n",
    "\n",
    "            points    += x.shape[0]\n",
    "            \n",
    "    # normalization if needed\n",
    "    params_true = params_true * (Y_max - Y_min) + Y_min\n",
    "    params_NN   = params_NN   * (Y_max - Y_min) + Y_min\n",
    "    \n",
    "    test_error = 100*np.mean(np.sqrt((params_true - params_NN)**2)/params_true,axis=0)\n",
    "    \n",
    "    RMSE = np.sqrt(np.mean((params_true - params_NN)**2,axis=0))\n",
    "    RMSE_P = RMSE*100\n",
    "    params_true_mean = np.mean(params_true)\n",
    "    tmp = np.mean((params_true - params_true_mean)**2, axis=0)\n",
    "    R2 = 1 - (RMSE)**2 / tmp\n",
    "    # print('Error Omega_m = %.3f'%test_error[0])\n",
    "    print(r' RMSE = %.3f'%RMSE[0])\n",
    "    print(r' $R^2$ = %.3f'%R2[0])\n",
    "    print('Error S_8 = %.3f'%test_error[0])\n",
    "\n",
    "\n",
    "    f, axarr = plt.subplots(1, 1, figsize=(5,5))\n",
    "    axarr.plot(np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),np.linspace(min(params_true[:,0]),max(params_true[:,0]),100),color=\"black\")\n",
    "\n",
    "    axarr.plot(params_true[:,0],params_NN[:,0],marker=\"o\",ls=\"none\",markersize=2, color=color)\n",
    "    axarr.set_xlabel(r\"True $S_8$\")\n",
    "    axarr.set_ylabel(r\"Predicted $S_8$\")\n",
    "    # axarr.text(0.1,0.9,\"%.3f %% error\" % test_error[0],fontsize=12,transform=axarr.transAxes)\n",
    "    \n",
    "    axarr.text(0.08,0.9,r\"RMSE = %.3f %% \" % RMSE_P[0],fontsize=12,transform=axarr.transAxes)\n",
    "    axarr.text(0.08,0.82,r\"$R^2$ = %.3f\" % R2[0],fontsize=12,transform=axarr.transAxes)\n",
    "    \n",
    "    \n",
    "    # axarr[1].plot(np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),np.linspace(min(params_true[:,1]),max(params_true[:,1]),100),color=\"black\")\n",
    "    # axarr[1].plot(params_true[:,1],params_NN[:,1],marker=\"o\",ls=\"none\",markersize=2)\n",
    "    # axarr[1].set_xlabel(r\"True $S_8$\")\n",
    "    # axarr[1].set_ylabel(r\"Predicted $S_8$\")\n",
    "    # axarr[1].text(0.1,0.9,\"%.3f %% error\" % test_error[1],fontsize=12,transform=axarr[1].transAxes)\n",
    "\n",
    "    if output:\n",
    "        f.savefig('./output/'+out_name+'.pdf', dpi=300, format='pdf')\n",
    "\n",
    "        # Also save for LFI later\n",
    "        info = dict()\n",
    "        info['params'] = params_true\n",
    "        info['compressed_DV'] = params_NN\n",
    "        np.save('./output/'+out_name+'_compressed_dv',info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb639fc0-7159-4859-9123-c6ec04a5f5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "Loss: 0.0069\n",
      "Val Loss: 0.0042\n",
      "Epoch 10/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0042\n",
      "Epoch 20/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0042\n",
      "Epoch 30/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0043\n",
      "Epoch 40/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0042\n",
      "Epoch 50/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0042\n",
      "Epoch 60/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0042\n",
      "Epoch 70/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0042\n",
      "Epoch 80/199\n",
      "----------\n",
      "Loss: 0.0062\n",
      "Val Loss: 0.0042\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m train_losses_tracker   \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m valid_losses_tracker   \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 47\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, dataset_size, val_size, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     46\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 47\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m epoch_val_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m val_size\n\u001b[1;32m     49\u001b[0m valid_losses_tracker\u001b[38;5;241m.\u001b[39mappend(epoch_val_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(608, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.layers(x)\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=10)\n",
    "\n",
    "train_losses_tracker   = []\n",
    "valid_losses_tracker   = []\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, len(train_y), len(val_y), optimizer, NUM_EPOCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12bbcc1-de27-4ee5-9d16-b00196e8be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_test_error(model, test_loader, out_name = 'test_3',Y_min=0, Y_max=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b61235-6a74-4acd-877e-bd8e5026fdca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9837c-57de-499a-9f85-bcc107834dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
